{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am fine\n"
     ]
    }
   ],
   "source": [
    "print(\"I am fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/llama2-py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/root/anaconda3/envs/llama2-py311/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:33<00:00, 11.18s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/120040051/hf_llama2\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"/120040051/hf_llama2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, tokenizer, model, device):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    # Tokenize the input sentence\n",
    "    encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Move encodings to the same device as the model\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    attention_mask = encodings.attention_mask.to(device)\n",
    "\n",
    "    # Calculate Negative Log-Likelihood\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids, attention_mask=attention_mask)\n",
    "        neg_log_likelihood = outputs.loss * input_ids.size(1)  # Multiply by sequence length\n",
    "\n",
    "    # Calculate Perplexity and append to list\n",
    "    ppl = torch.exp(neg_log_likelihood / input_ids.size(1))  # Divide by sequence length\n",
    "    perplexity = ppl.item()\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 sentences\n",
      "Processed 200 sentences\n",
      "Processed 300 sentences\n",
      "Processed 400 sentences\n",
      "Processed 500 sentences\n",
      "Processed 600 sentences\n",
      "Processed 700 sentences\n",
      "Processed 800 sentences\n",
      "Processed 900 sentences\n",
      "Processed 1000 sentences\n",
      "Processed 1100 sentences\n",
      "Processed 1200 sentences\n",
      "Processed 1300 sentences\n",
      "Processed 1400 sentences\n",
      "Processed 1500 sentences\n",
      "Processed 1600 sentences\n",
      "Processed 1600 sentences\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"tmp_compositional_sents.json\", 'r') as f:\n",
    "    j_data = json.load(f)\n",
    "\n",
    "sent_list = j_data[\"data\"]\n",
    "sent_list_ppl = []\n",
    "\n",
    "i = 0\n",
    "for sent_dict in sent_list:\n",
    "    text = sent_dict[\"text\"]\n",
    "    ppl = calculate_perplexity(text, tokenizer, model, device)\n",
    "    sent_dict[\"ppl\"] = round(ppl, 2)\n",
    "    sent_list_ppl.append(sent_dict)\n",
    "    i += 1\n",
    "    if (i % 100 == 0):\n",
    "        print(f\"Processed {i} sentences\")\n",
    "print(f\"Processed {i} sentences\")\n",
    "j_data[\"data\"] = sent_list_ppl\n",
    "\n",
    "outfile = \"sentence_table.json\"\n",
    "print(f\"Writing the results to {outfile}, ...\")\n",
    "with open(outfile, 'w') as f:\n",
    "    json.dump(j_data, f)\n",
    "print(\"Writing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.415712356567383\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"A man is eating pizza in the restaurant.\"\n",
    "perplexity_values = calculate_perplexity(sent1, tokenizer, model, device)\n",
    "print(perplexity_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
